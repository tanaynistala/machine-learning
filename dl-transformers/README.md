# CS137 DNN: Assiginment 4


## Objective 

The objective of this assignment is to give you a chance to understand the attention mechanism and to gain some experience of language modeling. 


## Instructions 

In this assignment, you have two tasks. The first task is to implement the multi-head attention layer with linear transformations and other elementry operations. 
Please check `mha_notebook.ipynb` for detailed instructions. 


In the second task, you need to implement a Transformer encoder with torch layers. Please check `language_modeling.ipynb` for detailed instructions. To load the WikiText-2 dataset, you need to install the `torchtext` package with version 0.16. You might need to use `pip install` instead of `conda install`. If necessary, you can use a compatible Torch version instead of our cs137 env.   


## Grading

Detailed grading rules are inside the notebook. Finally, you need to submit:
* your code files;
* your trained model (please use the default file name in the notebook).  

Please do not include the downloaded data file. Please zip the folder `assignment4` into a zip file and submit it. 

When we grade your code, we check your result notebook as well as your code. If your code cannot generate the result of a problem in your notebook file, you will get zero point for that problem. 


