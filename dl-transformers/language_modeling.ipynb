{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for Modeling Sentences\n",
    "\n",
    "**Q2 (10 points)** In this task, we will train a Small Language Model (SLM) based on the transformer architecture. The task is to predict the next character in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(180000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "import time\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The code for data loading is ready for you to use, so you don't need to make any changes to the following code block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m char_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(printable, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(printable)))) \u001b[38;5;66;03m# reverse table\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# turn each data subset to a 1-d array of numbers ranging in `range(len(printable))`\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m train_data \u001b[38;5;241m=\u001b[39m process_textdata(train_iter, char_dict)\n\u001b[1;32m     31\u001b[0m val_data \u001b[38;5;241m=\u001b[39m process_textdata(val_iter, char_dict)\n\u001b[1;32m     32\u001b[0m test_data \u001b[38;5;241m=\u001b[39m process_textdata(test_iter, char_dict)\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mprocess_textdata\u001b[0;34m(article_iter, char_dict)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mConcatenate a list of articles into a single string, remove non-printable chars from the string,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mand convert it to a np array.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([article \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m article_iter])\n\u001b[0;32m---> 17\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01min\u001b[39;00m printable, text))\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: char_dict[x], text)), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mprocess_textdata.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mConcatenate a list of articles into a single string, remove non-printable chars from the string,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mand convert it to a np array.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([article \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m article_iter])\n\u001b[0;32m---> 17\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01min\u001b[39;00m printable, text))\n\u001b[1;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: char_dict[x], text)), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import string\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def process_textdata(article_iter, char_dict):\n",
    "    \"\"\"\n",
    "    Concatenate a list of articles into a single string, remove non-printable chars from the string,\n",
    "    and convert it to a np array.\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\\n\".join([article for article in article_iter])\n",
    "    text = ''.join(filter(lambda x: x in printable, text))\n",
    "    data = torch.tensor(list(map(lambda x: char_dict[x], text)), dtype=torch.int32)\n",
    "\n",
    "    return data\n",
    "\n",
    "# load the WikiText-2 dataset\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "\n",
    "# define the vocabulary, which contains all printable characters.\n",
    "printable = list(string.printable)\n",
    "char_dict = dict(zip(printable, range(len(printable)))) # reverse table\n",
    "\n",
    "# turn each data subset to a 1-d array of numbers ranging in `range(len(printable))`\n",
    "train_data = process_textdata(train_iter, char_dict)\n",
    "val_data = process_textdata(val_iter, char_dict)\n",
    "test_data = process_textdata(test_iter, char_dict)\n",
    "\n",
    "\n",
    "# Some data exploration\n",
    "\n",
    "print('Data statistics:')\n",
    "print(f'Number of characters: {len(train_data)}(train), : {len(val_data)}(val), : {len(test_data)}(test)')\n",
    "\n",
    "uniq, uniq_counts = torch.unique(train_data, return_counts=True)\n",
    "total = torch.sum(uniq_counts)\n",
    "uniq = uniq.numpy()\n",
    "\n",
    "freq = (uniq_counts / total).numpy()\n",
    "ch_freq = dict([(printable[uniq[i]], freq[i]) for i in range(len(uniq))])\n",
    "for ch in string.printable:\n",
    "    if ch not in ch_freq:\n",
    "        ch_freq[ch] = 0\n",
    "\n",
    "print(\"Some characters' frequencies:\")\n",
    "print(\", \".join([f\"{ch}: {ch_freq[ch]:.3f}\" for ch in \"abcdefghijklmnopqrstuvwxyz\"]))\n",
    "\n",
    "ent = scipy.stats.entropy(freq, base=2.0)\n",
    "print(f\"The Shannon entropy of characters is {ent:.3f}, which means that the per-character cross-entropy loss of\" +\n",
    "      f\" a simple model (guessing the next character by frequencies) is {ent:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Transformer as a language model\n",
    "\n",
    "In the task below, you are supposed to train a transformer to model text data. Essentially your model defines the probability $p(y_t | y_{t-1}, \\ldots, y_{t - k})$. \n",
    "\n",
    "**(Q2 part 1, 5 points)** You are supposed to implement the transformer using [multi-head attention layers](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html). In particular, you should implement the [Transformer encoder](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html), but you need to turn on the causal flag in the forward calculation. Please check the documentation of `MultiheadAttention`. Note that the books states GPT as transformer decoder, but it is essentially the encoder with the causal flag on. \n",
    "\n",
    "**(Q2 part 2, 5 points)** You will implement the training code that trains the model with the given data. Your work is really similar to this [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html), so please read this tutorial carefully. However, there are two differences you may want to keep in mind. First, we do character-level language modeling. Second, you CANNOT use a Transformer model from Torch directly. Therefore, you can best use the idea in the tutorial if you have a good understanding of it.  \n",
    "\n",
    "\n",
    "**(Q2 part 3, 5 points)** Your model will be evaluated by per-character cross-entropy loss. You will get \n",
    "* 1 points if your per-character cross-entropy loss is less than 2.5 (a feedforward model defining a Markov model $p(y_t | y_{t-1})$ is able to achieve this number). \n",
    "* 4 points if your per-character cross-entropy loss is less than 2\n",
    "* 5 points if your per-character cross-entropy loss is less than 1.8\n",
    "\n",
    "\\*The performance from a [paper](https://arxiv.org/pdf/1808.04444.pdf) indicates that an LSTM can achieve performance of 1.43 * ln(2) = 0.991.  \n",
    "\\*The `zip` program for compressing files roughly can achieve a performances of 3.522 bits per character. It corresponds to a performance of  3.522 * ln(2) = 2.441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "You should implement your model class `SmallLanguageModel` and a function `train` to support the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, val loss 1.778340220974441s 0.37107219099998473\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SmallLanguageModel(\n",
       "  (emb): Embedding(100, 256)\n",
       "  (pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.01, inplace=False)\n",
       "  )\n",
       "  (enc): Sequential(\n",
       "    (0): EncoderLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (norm1): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (norm1): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (norm1): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (norm1): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=False)\n",
       "      )\n",
       "      (norm1): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (dense1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): Norm(\n",
       "        (dropout): Dropout(p=0.01, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from language_modeling import SmallLanguageModel, train\n",
    "\n",
    "model = SmallLanguageModel(vocabulary=printable).to(device)\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "train(model, train_data, val_data, loss_func, optimizer, scheduler, num_epochs=1, bptt = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and test it\n",
    "\n",
    "From this cell on, the code is used to test your model. You should NOT modify the code in this subsection. In particular, you should save your model using the default model name; otherwise, you will lose 2 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"my_slm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry-wise difference between the two calculations should be very small (below 1e-5). The difference from your model is:  4.3259934e-07\n"
     ]
    }
   ],
   "source": [
    "# Test whether the model has the same behavior when running sequentially or with batches\n",
    "\n",
    "model = torch.load(\"my_slm.pt\").to(device)\n",
    "\n",
    "# A example sentence:\n",
    "sen = \"\\nThis is a test case.\"\n",
    "sen_data = torch.tensor([char_dict[ch] for ch in sen], dtype=torch.long).view([-1, 1]).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = model(sen_data)\n",
    "\n",
    "    output2 = []\n",
    "    for i in range(sen_data.shape[0]):\n",
    "        out = model(sen_data[:(i+1)])\n",
    "        output2.append(out[-1])\n",
    "\n",
    "    output2 = torch.stack(output2, dim=0)\n",
    "\n",
    "    diff = torch.mean(torch.abs(output1 - output2)).cpu().numpy()\n",
    "\n",
    "print(\"The entry-wise difference between the two calculations should be very small (below 1e-5).\",\n",
    "      \"The difference from your model is: \", diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of chars in the test set is  1258600\n",
      "The per-char-loss is 1.790\n"
     ]
    }
   ],
   "source": [
    "# Test the per-character cross-entropy loss of your model\n",
    "from third_party import evaluate, batchify\n",
    "\n",
    "eval_batch_size = 10\n",
    "test_data = test_data.to(device)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "test_loss = evaluate(model, test_data, loss_func)\n",
    "\n",
    "print('The total number of chars in the test set is ', torch.numel(test_data))\n",
    "\n",
    "print('The per-char-loss is %.3f' % test_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to generate sentences\n",
    "\n",
    "Now we can use the trained model to generate text with a starting string. The naive model just predict frequent characters in the text, so there is no meaningful generation yet. You can provide different \"prompts\" and see what content the model will generate after that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from \"I hav\", the generated sentence is:\n",
      "\"I have of the conded the lating was the compert in a the win his hare the this oust s wo foun tereare act\"\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions as distributions\n",
    "\n",
    "def generate_text(model, start_string, char_list):\n",
    "    \"\"\" Generate random text from a starting string. \"\"\"\n",
    "\n",
    "    input_string = start_string\n",
    "    if len(input_string) == 0:\n",
    "        input_string = \"\\n\" # use the newline character as the BOS\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_int = [char_list.index(s) for s in start_string]\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 0.5\n",
    "\n",
    "    for i in range(num_generate):\n",
    "\n",
    "        input_tensor = torch.tensor(input_int, dtype = torch.long).view([-1, 1]).to(device)\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "        # remove the batch dimension\n",
    "        prediction = torch.softmax(outputs[-1, 0, :] / temperature, dim=0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        pred_int = int(distributions.Categorical(probs = prediction).sample())\n",
    "\n",
    "        # The calculation has a lot of repeatition because computation for the first part\n",
    "        # of the sequence is the same at every iteration. But it's fine for our example.\n",
    "        input_int.append(pred_int)\n",
    "        input_string = input_string + char_list[pred_int]\n",
    "\n",
    "\n",
    "    return input_string\n",
    "\n",
    "\n",
    "start_string = 'I hav'\n",
    "gen_sen = generate_text(model, start_string, printable)\n",
    "gen_sen = gen_sen.split('\\n')[0]\n",
    "\n",
    "print('Starting from \"' + start_string + '\", the generated sentence is:')\n",
    "print('\"' + gen_sen + '\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
